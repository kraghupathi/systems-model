#+TITLE:     Bootstrapping Steps for Cluster Setup
#+DATE:      2015-12-21 Mon 
#+PROPERTY: session *scratch*
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org
#+OPTIONS: ^:nil

* Introduction
  This document describes about how to setup cluster in AWS or in any
  collge/institute.

* IMPORTANT - Security Groups on amazon
  If you are working on AWS cluster, we have to create *Security
  group* for each server node.
  Security groups are as follows:
  1. [[./security-groups-on-amazon/Router-security-group.png][Router]]
  2. [[./security-groups-on-amazon/ansible-security-group.png][Config-server]]
  3. [[./security-groups-on-amazon/ads-security-group.png][ADS( Auto deployment Service)]]
  4. [[./security-groups-on-amazon/nagios-security-group.png][Nagios]]
  5. [[./security-groups-on-amazon/ossec-security-group.png][Ossec-server]]
  6. [[./security-groups-on-amazon/public-dns-security-group.png][Public DNS]]
  7. [[./security-groups-on-amazon/private-dns-security-group.png][Private DNS]]
  8. [[./security-groups-on-amazon/reverse-proxy-security-group.png][Reverse-Proxy]]
  9. [[./security-groups-on-amazon/rsnapshot-security-group.png][Rsnapshot]]
  10. [[./security-groups-on-amazon/rsyslog-security-group.png][Rsyslog-server]]
  11. [[./security-groups-on-amazon/production-lab.png][Production-lab]]
* Bootstrapping the cluster
  This bootstrapping steps are done based on ansible playbooks of the
  systems-model repository.
  
  - NOTE ::  While configuring network in servers, *HWADDR* (MAC address of
             interface) value is an optional or this can be ignored.

** Base machine Setup 
   Need a base machine to setup cluster where bootstrapping is being
   performed. The base machine can be either physical machine or VM/container
   based on avaible resources such as CPUs, RAM, Disk Space etc.
   
   The following basic hardware requirements are needed for base machine to
   setup cluster

   |----------------+--------------|
   | *Resource*     | *Fixed*      |
   |                | (or Minimum) |
   |----------------+--------------|
   | RAM            | 16GB         |
   | Disk           | 1TB          |
   | CPU            | 16 cores     |
   | I/O throughput | 1000 IOPS    |
   |----------------+--------------|

   Have direct internet access to base machine, Router and Ansible/config-server
   IPs. So that, need not worry about proxy settings issues if network uses any
   proxy.

** Setup bridged network
*** Connect LAN interface to Bridge (br0) 
    - Check your interfaces by running the command
      #+BEGIN_EXAMPLE 
      ifconfig -a
      #+END_EXAMPLE
      That gives you MAC address of interfaces(eth0, eth1, etc,.). Copy that.
    - Create *ifcfg-br0* file in =/etc/sysconfig/network-scripts/= and add the
      following fields
      #+BEGIN_EXAMPLE 
      DEVICE=br0
      BOOTPROTO=static
      ONBOOT=yes
      TYPE=Bridge
      NM_CONTROLLED=no
      IPADDR=<ip>
      NETMASK=255.255.252.0
      GATEWAY=<ip>
      DNS1=<ip>
      DNS2=<ip>
      #+END_EXAMPLE
    - Then create another *ifcfg-eth0* file in =/etc/sysconfig/network-scripts/=
      and add the following fields:
      #+BEGIN_EXAMPLE 
      DEVICE=eth0
      HWADDR=<<Hardware Address of eth0 interface>>
      TYPE=Ethernet
      ONBOOT=yes
      NM_CONTROLLED=no
      BOOTPROTO=none
      BRIDGE=br0
      #+END_EXAMPLE
      Paste the copied MAC address of the interface in HWADDR field
    - Restart the network using
      #+BEGIN_EXAMPLE 
      service network restart
      #+END_EXAMPLE
    - Now you can see ip assigned to br0 and you will be able to get internet.
*** Create bridge(br1) for private Network
    - Create *ifcfg-br1* file in =/etc/sysconfig/network-scripts/= and add the
      following fields:
      #+BEGIN_EXAMPLE 
      DEVICE=br1
      TYPE=Bridge
      ONBOOT=yes
      NM_CONTROLLED=no
      BOOTPROTO=none
      #+END_EXAMPLE
    - Restart network again to see the created bridge(br1)
      #+BEGIN_EXAMPLE 
      service network restart
      #+END_EXAMPLE
    - Perform the following steps
      #+BEGIN_EXAMPLE
      chkconfig NetworkManager off
      chkconfig network on
      #+END_EXAMPLE
    - Now you can see the bridges on your machine using *brctl show*
      command. That will give you the connections between interfaces.
    - Need to set proxy for all the containers if network uses any
      proxy.
    *Reference Link* for bridge setup
    https://github.com/vlead/ovpl/blob/master/docs/bridge-setup.org

** Setup base machine with OpenVZ etc. for ADS
   - Install git
    #+BEGIN_EXAMPLE
     yum -y install git
    #+END_EXAMPLE
   - Setup base machine server with the following steps
     + Clone the repository from GitHub
       #+BEGIN_EXAMPLE 
        git clone https://github.com/vlead/setup-ovpl-centos.git
       #+END_EXAMPLE
     + Change directory to =setup-ovpl-centos/scripts/=
       #+BEGIN_EXAMPLE 
	cd setup-ovpl-centos/scripts/
       #+END_EXAMPLE
     + Now we will setup base machine using =centos_prepare_ovpl.sh=
       file. Comment installation of following packages:
       -  git (line 35-41)
       -  mongodb (line 60-66)
     + In =install_dependencies.sh= file comment lines from 10 to 27.
     + After commenting run =centos_prepare_ovpl.sh=.
        #+BEGIN_EXAMPLE
        ./centos_prepare_ovpl.sh 
        #+END_EXAMPLE 
     + Running above script will install dependencies for openvz.
     + If you have latest ubuntu-12.04 template already as required by
       the ADS for lab hosting. Then that template can be copied to
       meta folder with the name "ubuntu-12.04-custom-x86_64.tar.gz",
       so that script will not download that template from Internet
     + Anyway, the above script downloads customized ubuntu template
       used for deploying labs from
       http://files.virtual-labs.ac.in/downloads/ads.
     + OPTIONAL :: If you don't have ADS required template, you can
                   create customized template with the following
                   steps:
		   
       1. Create a OpenVZ container with base or default ubuntu
          template. Default template should be in =/vz/template/cache=
          directory
	  #+BEGIN_EXAMPLE
	  vzctl create CTID --hostname <name of the container> --ostemplate <template-name without file extension> --ipadd <x.x.x.x>
	  Example:
	  vzctl create 100 --hostname custmize-ubuntu-template --ostemplate ubuntu-12.04-x86_64 --ipadd 192.168.1.3
	  #+END_EXAMPLE
       2. Start the container and enter into that container
	  #+BEGIN_EXAMPLE
	  vzctl start CTID
	  vzctl enter CTID
	  #+END_EXAMPLE
       3. Check the internet connection inside the container. If
          internet is not working, Configure networking for the
          container,if required using
          =/etc/sysconfig/network-scripts/ifcfg-eth0=
          etc. configuration files.
       4. Once the network is working, Install packages, services to run on
          start-up, create users if required.
       5. Stop the container
	  #+BEGIN_EXAMPLE
	  vzctl stop CTID
	  #+END_EXAMPLE
       6. Go to the private folder where container's files stored
          =/vz/private/<CTID>=.
       7. Create customized template container using 
	  #+BEGIN_EXAMPLE
	  tar cjf ubuntu-12.04-custom-x86_64.tar.gz *
	  #+END_EXAMPLE
       8. Move this template to =/var/template/cache/=
       9. Test the newly created template by creating new container to
          verify that it is working properly.
       10. *Reference Link* :
           https://www.sbarjatiya.com/notes_wiki/index.php/Modifying_openVZ_templates
     + Reboot the machine and choose openvz kernel from boot menu.
     + Enable automated addition to create interface to bridge using:
       #+BEGIN_EXAMPLE
       echo 'EXTERNAL_SCRIPT="/usr/sbin/vznetaddbr"' >> /etc/vz/vznet.conf
       #+END_EXAMPLE
     + Enable forwarding of ip packets, by editing =/etc/sysctl.conf= file as
       follows:
       #+BEGIN_EXAMPLE
       net.ipv4.ip_forward = 1
       #+END_EXAMPLE
     + Edit =/etc/vz/conf/ve-vswap-256m.conf-sample= to change value of
       NETFILTER to
       #+BEGIN_EXAMPLE
       NETFILTER="full"
       #+END_EXAMPLE
     + Reboot into openvz kernel.
     + Download the latest centos template from below link and place at =/vz/template/cache=
       http://download.openvz.org/template/precreated/centos-6-x86_64.tar.gz
*** (OPTIONAL) Move /vz to a partition where there is enough space
    + By default all OpenVZ container information will go into =/vz= partition.
    + Some times we may not have enough space for =/vz= partition, so it is good
      to move this partition to where enough space available. Then symlink from
      the new location to =/vz=.
      #+BEGIN_EXAMPLE
      mv /vz /mnt/sda2/openvz_files
      ln -s /mnt/sda2/openvz_files /vz
      #+END_EXAMPLE
*** Troubleshooting VM OpenVZ issues
    In case the setup is performed in a VM and VM is unstable such that
    containers fail to stop and VM fails to reboot.  Then following changes may
    not help:
    - Reducing OpenVZ kernel version (downgrading)
    - Removing tboot
    - Update VM OS to latest packages Most probably issue would be with
      container template.  Download fresh template directly from openvz.org
      mirrors.
** Create all servers/nodes
   - Create a file called =cluster.sh= and add the following shell
     commands to it. Update *cluster* variable value based on cluster
     name. Also update bridges variables *public_bridge* and
     *private_bridge* accordingly. This shell script creates all
     containers and they would be in running state.
     #+BEGIN_EXAMPLE 
     cluster="base5"
     public_bridge="br0"
     private_bridge="br1"
     for i in {1001..1010}
     do
     vzctl create $i  --ostemplate centos-6-x86_64
     if [ $i == 1001 ]
     then
     vzctl set $i --hostname router.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth0,,,,$public_bridge --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     elif [ $i == 1002 ]
     then 
     vzctl set $i --hostname ansible.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth0,,,,$public_bridge --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     elif [ $i == 1003 ]
     then
     vzctl set $i --hostname ossec-server.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     
     elif [ $i == 1004 ]
     then
     vzctl set $i --hostname rsyslog.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     elif [ $i == 1005 ]
     then
     vzctl set $i --hostname privatedns.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     
     elif [ $i == 1006 ]
     then
     vzctl set $i --hostname publicdns.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     elif [ $i == 1007 ]
     then
     vzctl set $i --hostname reverseproxy.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     elif [ $i == 1008 ]
     then
     vzctl set $i --hostname nagios.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     elif [ $i == 1009 ]
     then
     vzctl set $i --hostname ads.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --diskspace 20G --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     elif [ $i == 1010 ]
     then
     vzctl set $i --hostname rsnapshot.$cluster.vlabs.ac.in --save
     vzctl set $i --netif_add eth1,,,,$private_bridge --save
     vzctl start $i
     vzctl set $i --onboot yes --save
     fi
     done
     #+END_EXAMPLE
   - Add executable permission to =cluster.sh= script and run script as:
     #+BEGIN_EXAMPLE
     chmod +x cluster.sh
     ./cluster.sh
     #+END_EXAMPLE
** Router
   - Configure the network on the router, it needs to be connected
     with two bridges br0 and br1 in order to get internet connection
     and to setup private network via this router. Run the following
     commands.
     + NOTE :: *No need to do below steps again here. These
               steps* *already taken care by =cluster.sh= script. Just
               for *understanding we gave examples.*
     #+BEGIN_EXAMPLE 
      vzctl set 1001 --netif_add eth0,,,,br0  --save 
      vzctl set 1001 --netif_add eth1,,,,br1 --save
      vzctl set 1001 --onboot yes --save
     #+END_EXAMPLE
     It means, the container is created with two interfaces eth0 and
     eth1. eth0 of the router is connected to br0 of host machine and
     eth1 of the router is connected to br1.
   - Enter into the router
     #+BEGIN_EXAMPLE 
     vzctl enter 1001
     #+END_EXAMPLE
   - Set root passwd, and make note of it in some file.
     #+BEGIN_EXAMPLE
     passwd
     #+END_EXAMPLE
*** Inside the Router 
    Now you may not get internet to the router. To get that do the following
    steps.

    We need two IPs to setup router machine one is for accessing labs and
    servers over http, https and dns and another is for private network ip. This
    private ip is for gateway to all the nodes and lab containers.
    
    - Find one available ip in your network using ping command, to assign it to
      the router eth0 interface. This ip will become public ip for router.
    - Create *ifcfg-eth0* file in =/etc/sysconfig/network-scripts/= and add the
      following fields. In place of HWADDR add MAC address of eth0 interface.
      #+BEGIN_EXAMPLE 
      DEVICE=eth0
      TYPE=Ethernet
      HWADDR=<MAC address of the eth0 interface>
      BOOTPROTO=static
      ONBOOT=yes
      NM_CONTROLLED=no
      IPADDR=<ip-address>
      NETMASK=<netmask>
      GATEWAY=<gateway>
      DNS1=<external-dns1>
      DNS2=<external-dns2>
      #+END_EXAMPLE
    - Create another =ifcfg-eth1= file in =/etc/sysconfig/network-scripts/=.
      This one is actually for creating private network. It acts like a Gateway
      to all other containers
      #+BEGIN_EXAMPLE 
      DEVICE=eth1
      HWADDR=<<Hardware address of eth1 interface>>
      BOOTPROTO=static
      ONBOOT=yes
      NM_CONTROLLED=no
      IPADDR=10.100.1.1
      NETMASK=255.255.252.0
      #+END_EXAMPLE
    - Restart the network 
      #+BEGIN_EXAMPLE
       service network restart
      #+END_EXAMPLE
    - NOTE ::  Check if you are able to access internet. If not then the
      gateway might be set to default value, not the one we mentioned
      in config file. Comment =networking= and =gateway= parameter in
      =/etc/sysconfig/network= file as follows, and restart the network.
      
      #+BEGIN_EXAMPLE
      #NETWORKING="yes"
      #GATEWAYDEV="venet0"
      NETWORKING_IPV6="yes"
      IPV6_DEFAULTDEV="venet0"
      HOSTNAME="router.{cluster}.vlabs.ac.in"
      #+END_EXAMPLE

     If this approach does not work then download the fresh centos
     template from openvz and create router again.

*** Add NAT rule and Edit =/etc/sysctl.conf= file
    - Write a temporary NATing rule to get internet in private network
      containers and This rule will be used to run playbooks of private and
      public dns nodes and router nodes smoothly.
      #+BEGIN_EXAMPLE 
      iptables -t nat -A POSTROUTING ! -d 10.100.0.0/22 -o eth0 -j SNAT --to-source <router_public_ip>
      iptables-save > /etc/sysconfig/iptables
      #+END_EXAMPLE
    - Edit/open the file =/etc/sysctl.conf= and modify the following line:
      #+BEGIN_EXAMPLE 
      net.ipv4.ip_forward = 1
      #+END_EXAMPLE
    - Restart the router container in order to enable ip forwarding
      #+BEGIN_EXAMPLE
      reboot
      #+END_EXAMPLE
    - Exit from container
    - *Reference Link* :
      https://openvz.org/Using_NAT_for_container_with_private_IPs
** Config-server node
*** Enter Into the container
    #+BEGIN_EXAMPLE
    vzctl enter 1002
    #+END_EXAMPLE
*** 
*** Setup container
    - We need two IPs to setup ansible machine one is for public access over ssh
      and another is for private network ip. Private ip is for configuring other
      nodes as well as itself and internal connections.
      + NOTE :: *No need to do below steps again here. These steps*
                *already taken care by =cluster.sh= script. Just for
                *understanding we gave examples.*

      #+BEGIN_EXAMPLE
      vzctl set 1002 --netif_add eth0,,,,br0 --save
      vzctl set 1002 --netif_add eth0,,,,br1 --save
      vzctl set 1002 --onboot yes --save
      #+END_EXAMPLE
    - To configure the network do the same steps as followed for router node
      [[Inside the Router]], set IPADDR and MAC addresses of the interfaces
      appropriately. Set private IP as 10.100.1.2 for ansible container.
    - Enter inside the container.
      #+BEGIN_EXAMPLE
      vzctl enter 1002
      #+END_EXAMPLE
    - Install openssh if not installed. By default some distro comes
      with openssh. If the package is not installed and restart the
      ssh service.
      #+BEGIN_EXAMPLE 
      yum install openssh -y
      service sshd start
      chkconfig sshd on
      #+END_EXAMPLE
    - Set root passwd
      #+BEGIN_EXAMPLE
       passwd
      #+END_EXAMPLE
    - Create a 'vlead' user.
      #+BEGIN_EXAMPLE
      adduser vlead 
      passwd vlead
      su - vlead
      ssh-keygen -t rsa
      #+END_EXAMPLE
    - Enable ssh access from localhost to localhost and router using key-based
      authentication
     #+BEGIN_EXAMPLE
     ssh-copy-id root@localhost  #from vlead user
     ssh-copy-id root@10.100.1.2  #from vlead user
     ssh-copy-id root@10.100.1.1  #from vlead user
     #+END_EXAMPLE
    - Note :: If the root user's ssh keys were copied earlier then the root
              user's private and public key can be placed inside the vlead
              user's .ssh directory, so that from vlead user we can login to
              other machine roots account.
    - Setup epel-release repo on the system for installing ansible"
     #+BEGIN_EXAMPLE 
     yum -y install epel-release  #from root
     yum -y install ansible       #from root
     #+END_EXAMPLE

*** Version control setup
    - Setup version control on ansible machine. In this node, git command is
      used to pull the updates from master branch of the systems model. From
      this node we do not push the updated files/folders to systems-model
      repository.
      #+BEGIN_EXAMPLE
      yum install git -y
      #+END_EXAMPLE
    - Create .ssh/config file in vlead user with following content, make sure
      permissions of the file are set to =400=.
      #+BEGIN_EXAMPLE
      GSSAPIAuthentication no
      #+END_EXAMPLE
    - If machine does not have direct internet access then you may need to add
      following lines also in .ssh/config file.
      #+BEGIN_EXAMPLE
      Host bitbucket.org
      HostName altssh.bitbucket.org
      Port 443
      User git
      #+END_EXAMPLE
    - Copy ssh public keys of vlead user to systems-model repository in
      bitbucket. 
      + Steps to add vlead user's public key to systems-model on BitBucket
	1.login as admin in bitbucket 
        2. Go to sytems-model repo 
        3. Go to settings 
        4. Add vlead user's public key in the deployment keys section.
        5. Then clone the systems-model repository inside 'vlead' user
           home directory by using the following command.
      #+BEGIN_EXAMPLE
      git clone git@bitbucket.org:vlead/systems-model.git     #from vlead user git
      #+END_EXAMPLE
    - Checkout develop or other appropriate branch.
      #+BEGIN_EXAMPLE
      git checkout develop   or  git checkout <branch-name> # Currently full updates are in develop only
      #Need to merge these changes to master very soon
      #+END_EXAMPLE
*** Setup Emacs and org-mode-8
    - Install emacs from vlead user only without becoming root by issuing
      following command.
      #+BEGIN_EXAMPLE
      ssh root@localhost yum -y install emacs 
      #+END_EXAMPLE
   
    - Setup org-mode using (for VLEAD user):
      #+BEGIN_EXAMPLE
      mkdir -p ~/emacs/lisp   #as vlead user
      cd ~/emacs/lisp
      wget http://orgmode.org/org-8.2.10.tar.gz
      tar zxvf org-8.2.10.tar.gz
      rm -rf org-8.2.10.tar.gz
      #+END_EXAMPLE

*** Run make and check syntax of site.yaml
    - Change directory to cloned repository systems-model and change the values
      of following variables in makefile
      + *ROUTER_IP* - Router's public IP.
      + *CONFIG_SERVER* - Ansible/Config server'e public IP.
      + *CLUSTER* - Name of the cluster
      + *SMTP_SMART_HOST* - Domain name of mail server/SMTP server.
    - And then run make
      #+BEGIN_EXAMPLE 
      cd ~/systems-model/
      make
      #+END_EXAMPLE

      Running make will create following hosts file:
      #+BEGIN_SRC text :tangle hosts 
      [ansible_server]
      #multiple-ok
      10.100.1.2
      
      [ossec_server]
      #Only one allowed
      10.100.1.3
      
      [public_dns]
      #multiple ok
      10.100.1.6
      
      [private_dns]
      #multiple ok
      10.100.1.5
      
      [rsyslog_server]
      #multiple ok
      10.100.1.4
      
      [reverseproxy_server]
      #multiple ok
      10.100.1.7
      
      [router]
      10.100.1.1
      
      [nagios_server]
      10.100.1.8
      
      [ads_server]
      10.100.1.9
      
      [rsnapshot_server]
      10.100.1.10
      #+END_SRC

    - Good to have Copy of generated cluster code =build/{cluster}= to another
      location. This is important as otherwise running make again will overwrite
      all changes since last time. For example, cluster name is cluster, then
      copy the =build/cluster= directory to =~/root/=
      #+BEGIN_EXAMPLE
      cp -r build/cluster ~/root/
      #+END_EXAMPLE
    - Check syntax of ansible playbooks.
      #+BEGIN_EXAMPLE
      cd ~/systems-model/build/{cluster}
      ansible-playbook -i hosts --list-tasks --syntax-check site.yaml
      #+END_EXAMPLE
    - Exit from container
** DNS servers and Configure
   - Enter into the containers one after the other and do the following steps
     + Configure the network-interface in
       =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following fields
       #+BEGIN_EXAMPLE 
        DEVICE=eth1
        HWADDR=<<Hardware address of eth1 interface>>
        BOOTPROTO=static
        ONBOOT=yes
        NM_CONTROLLED=no
        IPADDR=10.100.1.5/6(private/public)
        GATEWAY=10.100.1.1
        NETMASK=255.255.252.0
       #+END_EXAMPLE
     + Restart the network of dns nodes
       #+BEGIN_EXAMPLE 
       service network restart
       #+END_EXAMPLE
     + Set root password for both vms/containers
       #+BEGIN_EXAMPLE
       passwd
       #+END_EXAMPLE
     + Enable ssh access from ansible using key-based authentication
       (=ssh-copy-id= from vlead user)
   - We need working DNS in new containers.  If working DNS are obtained due to
     inherit feature of OpenVZ then it is fine. Otherwise we should use
     temporary external dns server's IP other than cluster's dns server to
     configure these nodes using ansible playbooks.  For example edit
     /etc/resolv.conf file as follows:
     #+BEGIN_EXAMPLE
     nameserver 4.2.2.2
     nameserver 8.8.8.8
     #+END_EXAMPLE
   - Comment =rsnapshot_client= and =ossec_client= roles in private-dns.yaml and
     public-dns.yaml
   - Run *public_dns.yaml* and *private_dns.yaml* play books on config-server.
      #+BEGIN_EXAMPLE 
      ansible-playbook -i hosts public_dns.yaml
      #+END_EXAMPLE
      #+BEGIN_EXAMPLE 
      ansible-playbook -i hosts private_dns.yaml
      #+END_EXAMPLE
   - Once the nodes are configured using playbooks, test whether the dns nodes
     are functioning properly or not from both private and public DNS nodes by
     issuing the following commands:
      #+BEGIN_EXAMPLE
      nslookup www.google.co.in 10.100.1.5
      nslookup router.[{cluster}.]vlabs.ac.in 10.100.1.6
      nslookup router.[{cluster}.]vlabs.ac.in 10.100.1.5
      #+END_EXAMPLE
   - Exit from container
** Configure Router 
   - In order to apply proper firewall rules for our cluster we need to
     configure router after dns servers are configured.
   - Comment =ossec_client= and =rsnapshot_client= roles in router.yaml
   - Enable ssh access from config-server node
     #+BEGIN_EXAMPLE
      ansible-playbook -i hosts router.yaml
     #+END_EXAMPLE
     - NOTE :: Reason to run router.yaml play book here is: We added NATing rule
               as temporary firewall rule. So in common role there is task which
               restarts iptables service, temporary rule will be no more. If we
               set POSTROUTING firewall rule as permanent rule then no need to
               run router.yaml file after dns servers are configured.
   - Test the router by issuing following command from router node itself.
     #+BEGIN_EXAMPLE
     nslookup router.[{cluster}.].vlabs.ac.in <router-public-ip>
     #+END_EXAMPLE
     This query should resolve to router public IP itself. 
   - Exit from container
** Ossec server
   - Enter into the ossec
     #+BEGIN_EXAMPLE 
     vzctl enter 1003
     #+END_EXAMPLE
   - Configure the network-interface in
     =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following fields
     #+BEGIN_EXAMPLE 
     DEVICE=eth1
     HWADDR=<<Hardware address of eth1 interface>>
     BOOTPROTO=static
     ONBOOT=yes
     NM_CONTROLLED=no
     IPADDR=10.100.1.3
     GATEWAY=10.100.1.1
     NETMASK=255.255.252.0
     #+END_EXAMPLE
   - Restart the network
     #+BEGIN_EXAMPLE 
     service network restart
     #+END_EXAMPLE
   - Set root password and enable ssh access from ansible using key-based
     authentication
   - Exit from container
** rsyslog server
   - Enter into the rsyslog container
     #+BEGIN_EXAMPLE 
     vzctl enter 1004
     #+END_EXAMPLE
   - Configure the network-interface in
     =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following fields
     #+BEGIN_EXAMPLE 
     DEVICE=eth1
     HWADDR=<<Hardware address of eth1 interface>>
     BOOTPROTO=static
     ONBOOT=yes
     NM_CONTROLLED=no
     IPADDR=10.100.1.4
     GATEWAY=10.100.1.1
     NETMASK=255.255.252.0
     #+END_EXAMPLE
   - Restart the network 
     #+BEGIN_EXAMPLE 
     service network restart
     #+END_EXAMPLE
   - Set root password and enable ssh access from ansible using key-based
     authentication.
   - Exit from container
** Reverse-proxy server
   - Enter into the reverseproxy container
     #+BEGIN_EXAMPLE 
     vzctl enter 1007
     #+END_EXAMPLE
   - Configure the network-interface in
     =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following fields
      #+BEGIN_EXAMPLE 
      DEVICE=eth1
      HWADDR=<<Hardware address of eth1 interface>>
      BOOTPROTO=static
      ONBOOT=yes
      NM_CONTROLLED=no
      IPADDR=10.100.1.7
      GATEWAY=10.100.1.1
      NETMASK=255.255.252.0
      #+END_EXAMPLE
   - Restart the network 
     #+BEGIN_EXAMPLE 
     service network restart
     #+END_EXAMPLE
   - Set root password and enable ssh access from ansible using key-based
     authentication
   - Private DNS must be setup completely before reverseproxy is created.
*** OPTIONAL ::  Enable secure connections(ssl/443)
**** Purchased ssl certificate 
     - If you already have purchased ssl certificates, configure
       =/etc/httpd/conf.d/ssl.conf= file with appropriate path for
       =SSLCertificateFile= and =SSLCertificateKeyFile= directives/fields
     - Restart httpd service if apache syntax is correct
       #+BEGIN_EXAMPLE
       apachectl -S         # Apache configuration syntax check
       service httpd restart
       #+END_EXAMPLE
   - Exit from container      
**** Self signed ssl certificates
     Use self signed certificates if purchased ssl certificates are not
     available.
     - Create a directory called =ssl= in =/etc/httpd/=
       #+BEGIN_EXAMPLE
       mkdir /etc/httpd/ssl
       #+END_EXAMPLE
     - Generate self signed certificate as follows:
       #+BEGIN_EXAMPLE
       openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/httpd/ssl/apache.key -out /etc/httpd/ssl/apache.crt
       #+END_EXAMPLE
     - Open the file =/etc/httpd/conf.d/ssl.conf=. Look for =SSLCertificateFile=
       and =SSLCertificateKeyFile= directives/fields and provide full path of
       certificates created above to those directives.
     - Restart httpd service if apache syntax is correct
       #+BEGIN_EXAMPLE
       apachectl -S         # Apache configuration syntax check
       service httpd restart
       #+END_EXAMPLE
     - Exit from container      
** Rsnapshot server
   Rnsapshot server is for taking backup of necessary files and folders from
   required nodes of the cluster.
   - Enter into the rsnapshot container
     #+BEGIN_EXAMPLE 
     vzctl enter 1010
     #+END_EXAMPLE
   - Configure the network-interface in
     =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following fields
     #+BEGIN_EXAMPLE 
     DEVICE=eth1
     HWADDR=<<Hardware address of eth1 interface>>
     BOOTPROTO=static
     ONBOOT=yes
     NM_CONTROLLED=no
     IPADDR=10.100.1.10
     GATEWAY=10.100.1.1
     NETMASK=255.255.252.0
     #+END_EXAMPLE
   - Restart the network 
     #+BEGIN_EXAMPLE 
     service network restart
     #+END_EXAMPLE
   - set root password and enable ssh access from ansible using key-based
     authentication
   - Exit from container
** Monitoring Server (Nagios)
   - Enter into the nagios container
     #+BEGIN_EXAMPLE 
     vzctl enter 1008
     #+END_EXAMPLE
   - Configure the network-interface in
     =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following fields
      #+BEGIN_EXAMPLE 
      DEVICE=eth1
      HWADDR=<<Hardware address of eth1 interface>>
      BOOTPROTO=static
      ONBOOT=yes
      NM_CONTROLLED=no
      IPADDR=10.100.1.8
      GATEWAY=10.100.1.1
      NETMASK=255.255.252.0
      #+END_EXAMPLE
   - Restart the network
     #+BEGIN_EXAMPLE 
     service network restart
     #+END_EXAMPLE
   - Set root password and enable ssh access from ansible using key-based
     authentication
   - Exit from container
** ADS server
   - Enter into ADS node
     #+BEGIN_EXAMPLE 
     vzctl enter 1009
     #+END_EXAMPLE
   - Configure the network-interface in
     =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following fields
     #+BEGIN_EXAMPLE 
     DEVICE=eth1
     HWADDR=<<Hardware address of eth1 interface>>
     BOOTPROTO=static
     ONBOOT=yes
     NM_CONTROLLED=no
     IPADDR=10.100.1.9
     GATEWAY=10.100.1.1
     NETMASK=255.255.252.0
     #+END_EXAMPLE
   - Restart the network 
     #+BEGIN_EXAMPLE 
     service network restart
     #+END_EXAMPLE
   - Set root passwd and enable ssh access from ansible using key-based
     authentication
   - Install git
     #+BEGIN_EXAMPLE
     echo "nameserver 8.8.8.8" > /etc/resolv.conf
     yum -y install git
     #+END_EXAMPLE
   - Generate root users ssh-keys. Setup trust based ssh from ads container root
     user to Base machine.
     #+BEGIN_EXAMPLE
     ssh-keygen -t rsa
     ssh-copy-id root@<base-machine-ip>
     #+END_EXAMPLE
   - Add ssh forced command in config server node inside =.ssh/authorized_keys=
     file of vlead user. Replace <public-key> with public key of ads machine.
     *Note:* This has to be done in config server.
     #+BEGIN_EXAMPLE
     command="hooks.sh",no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty <public-key>
     #+END_EXAMPLE
   - Make sure =/home/vlead/.ssh/authorized_keys= file has =600= permissions.
     #+BEGIN_EXAMPLE
     chmod 600 .ssh/authorized_keys
     #+END_EXAMPLE
   - Edit =~/.ssh/known_hosts= file and known hosts entry for ansible
     machine.  Replace <public-key-of-ansible-machine> with public key
     of vlead user in ansible machine.
     #+BEGIN_EXAMPLE
     ansible.{{cluster}}.vlabs.ac.in <public-key-of-ansible-machine>
     10.100.1.2 <public-key-of-ansible-machine>
     ansible.{{cluster}}.virtual-labs.ac.in <public-key-of-ansible-machine>
     #+END_EXAMPLE
   - Clone setup-ovpl-centos using:
     #+BEGIN_EXAMPLE
     git clone https://github.com/vlead/setup-ovpl-centos.git
     #+END_EXAMPLE
   - Install dependencies as follows
     #+BEGIN_EXAMPLE
     cd setup-ovpl-centos/scripts
     vim config.sh #set proxy settings here if network uses any proxy
     vim centos_prepare_ovpl.sh  #Comment OpenVZ installation and git installation lines 35-41 and 52-58
     ./centos_prepare_ovpl.sh    # Relax for 10 to 15 minutes or Go for a TEA
     #+END_EXAMPLE 
   - Terminate the starting of mongodb service process by pressing "ctrl+c".
   - Do following steps
     #+BEGIN_EXAMPLE 
     cd ovpl/config
     cp sample_config.json config.json
     cp sample_authorized_users.py authorized_users.py
     cd adapters
     cp sample_centos_bridged_config.py centos_bridged_config.py
     cp sample_base_config.py base_config.py
     cd ..
     #+END_EXAMPLE
   - Edit =ovpl/config/config.json= as follows:
     + COOKIE_SECRET :: Change to some other random string
     + APP_URL :: Update app url to include FQDN
	   *Example: ads.{cluster-name}.vlabs.ac.in:8080*
     + ADAPTER_TO_USE :: {"POOLID" : 1, "ADAPTERID" : 3 } 
       - In case of other types of setup use appropriate poolid or
         adapter Id.
     + LOGSERVER_CONFIGURATION / SERVER_IP :: "10.100.1.9"

   - Edit =ovpl/src/adapters/centos_bridged_config.py= as follows:
     + SUBNET_BRIDGE :: "br1"
   - Edit adapters/base_config.py as follows:
     + ADS_ON_CONTAINER = True
     + SUBNET = ["10.100.2.0/24"]
     + BASE_IP_ADDRESS = "root@10.4.14.202"  
       + Replace 10.4.14.202 with correct base machine (ie VM) IP
     + ADS_SERVER_VM_ID = "1009"
       + CTID of ADS container.  This is used while copying cloned
         labs and OVPL code from ADS container to lab container.  Note
         that git clone of lab happens within ADS container and not
         directly in lab container.
     + HOST_NAME = "{cluster}.vlabs.ac.in"
       + Domain name of lab-id
     + Set ADS_USING_HOOKS to 'True', to use ads-hooks service.
     + Set SERVICE_HOST="vlead@ansible.{cluster}.vlabs.ac.in"
   - Install dependencies 
     This assumes apache server is installed. If not, please install
     the apache server.
     #+BEGIN_SRC 
     sudo su -
     yum update -y
     yum install httpd -y
     yum install epel-release -y
     yum install "mod_wsgi" -y
     yum install python-pip
    
     cd /root/ovpl/src/ads-web-app
     python setup.py install

     rsync -avz --progress /root/ovpl/src/ads-web-app /var/www/html/
     chmod -R 777 /var/www/html/ads-web-app

     #+END_SRC

   - Configure Apache server virtual-host (httpd)

     Configure Apache to load mod_wsgi module and your project in
     VirtualHost (in /etc/httpd/conf/httpd.conf) Add below lines to
     =/etc/httpd/conf/httpd.conf=
     #+BEGIN_SRC 
    ServerName ads.vlabs.ac.in

    WSGIScriptAlias / /var/www/html/ads-web-app/app.wsgi
    WSGIScriptReloading On

    <Directory /var/www/html/ads-web-app>
    Order deny,allow
    Allow from all
    </Directory>

     #+END_SRC

   - Setup Google Oauth credentials
	
     Create a project at Google Dashboard and create Oauth
     credentails.  Provide *CONSUMER_KEY* and *CONSUMER_SECRET* in
     =/var/www/html/ads-web-app/config.py=.
     #+BEGIN_EXAMPLE
     CONSUMER_KEY = "<consumerkey>"
     CONSUMER_SECRET = "<consumer secret key>"
     #+END_EXAMPLE
   - Edit =/var/www/html/ads-web-app/config.py= as follows:
     + Put bunch of gmail IDs separated by , as shown in the example
       config in AUTHORIZED_USERS section.  While deploying labs Gmail
       OpenID login will be used.  Also the given gmail ID may have to
       be associated with a persona account.
   - Restart httpd service
     #+BEGIN_EXAMPLE
     service httpd restart
     #+END_EXAMPLE
   - Run services using
     #+BEGIN_EXAMPLE
     cd ~/ovpl
     ./manage_services.sh start
     #+END_EXAMPLE
    - Set nameserver for ads server to use private dns. Edit =/etc/resolv.conf=
      file as follows:
      #+BEGIN_EXAMPLE
      search <cluster>.virtual-labs.ac.in <cluster>.vlabs.ac.in
      nameserver 10.100.1.5 
      #+END_EXAMPLE
   - Exit from container
   - Reference
     + [[https://github.com/vlead/ovpl/blob/master/README.org][ADS setup]]
** Run site.yaml from config-server
    - Check management_ips in common_vars role and update them appropriately.
    - Add servers internal ips to =ossec_client_ips= variable in common_vars
      role.
    - Set private dns ips and private-dns-zone as follows, and comment
      =private-dns-ips: none=, =private-dns-zone: none= in common_vars role.
      #+BEGIN_EXAMPLE
      private_dns_ips: 
          - 10.100.1.5
      private_dns_zone: "{{prefix}}virtual-labs.ac.in {{prefix}}vlabs.ac.in"
      #+END_EXAMPLE
    - Comment =ossec_server.yaml= in =site.yaml=
    - Comment =ossec_client= and uncomment =rsnapshot_client= in all servers
      playbook.
    - Check syntax before running the =site.yaml=.
      #+BEGIN_EXAMPLE
      ansible-playbook -i hosts --list-tasks --syntax-check site.yaml
      #+END_EXAMPLE
    - Run site.yaml file. If you get any error try to run it again, the error
      may resolve.
      #+BEGIN_EXAMPLE 
      ansible-playbook -i hosts site.yaml
      #+END_EXAMPLE
** Host labs using ads service
   - Deploy a few labs using ads service to test the whole setup. Open
     http://ads.{cluster}.vlabs.ac.in:8080 and provide required fields.
     - How to host a lab :: 
       1. Open the browser (Preferably latest Firefox)
       2. Browse the link http://ads.{cluster}.vlabs.ac.in:8080
       3. Provide persona account details and log in 
       4. After logged in you will see the web page showing with
          three fields:
	  + *Lab ID* - In this field provide lab_d(e.g cse03-iiith, ccnsb03-iitk
            etc.)
	  + *Repo URL* - Provide lab GitHub repository URL.  Example:
            https://github.com/Virtual-Labs/problem-solving-iiith.git
	  + *Tag* - This is optional field, we can ignore.
       5. Then click on *Submit* button to host the lab in cluster
       6. If everything goes well, you will get an IP of lab where
          lab has been hosted.
	  - Note :: You can see the logs of lab hosting process in ads server
                    using
		    #+BEGIN_EXAMPLE
		    tail -f /root/logs/ovpl.log
		    #+END_EXAMPLE
   - Check whether the lab page has been serving or not using provided lab url
     using any browser.
** On Base machine
   - Set DNS on all containers to private DNS 10.100.1.5 IP using
     #+BEGIN_EXAMPLE
     vzctl set <CTID> --nameserver 10.100.1.5 --save     
     #+END_EXAMPLE
   - Stop all containers and reboot base machine.  Verify that things
     automatically work after reboot. If nameserver is not set to 10.100.1.5 in
     all the containers, we have to do the *Reboot related steps*.
*** Reboot related steps
    After every full reboot we need to do following to make cluster work
    + Set private DNS to 10.100.1.5 in all containers
     #+BEGIN_EXAMPLE
      vzctl set <CTID> --nameserver 10.100.1.5 --save     
      #+END_EXAMPLE
    + Restart httpd in reverseproxy

* To Do
- Add bootstrapping steps for creating vlabs.ac.in landing page, analytics
  server and lab-rsyslog server.
